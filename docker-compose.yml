version: '3.8'

services: # services refer to the scripts than run from docker-compose run. docker-compose run --rm compute-metrics. docker-compose run --rm process-candles
  compute-metrics:
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.spark # build the container using dockerfile.spark
    volumes:
      - ./spark:/app/spark # link code from local machine into docker container. edits locally edit the container.
      - ./.env:/app/.env
    env_file:
      - .env # load environment variables from .env
    environment:
      - SPARK_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569
      - PYTHONPATH=/app # environment variables for paths like python
    entrypoint: []
    command: # when the container starts, run the following command. this is how compute_metrics gets run.
      - /bin/bash
      - -c
      - |
        spark-submit \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569 \
          spark/jobs/compute_metrics.py

  process-candles:
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.spark
    volumes:
      - ./spark:/app/spark
      - ./.env:/app/.env
    env_file:
      - .env
    environment:
      - SPARK_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569
      - PYTHONPATH=/app
    entrypoint: []
    command: 
      - /bin/bash
      - -c
      - |
        spark-submit \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569,org.postgresql:postgresql:42.7.1 \
          spark/jobs/process_candles.py
          
  generate-report:
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.spark
    volumes:
      - ./spark:/app/spark
      - ./.env:/app/.env
    env_file:
      - .env
    environment:
      - SPARK_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569
      - PYTHONPATH=/app
    entrypoint: []
    command: 
      - /bin/bash
      - -c
      - |
        spark-submit \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569 \
          spark/jobs/generate_report.py

  ingest-candles: 
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.flask
    volumes: # propagate changes from local machine to docker container
      - ./app:/app/app
      - ./ingestion:/app/ingestion
      - ./.env:/app/.env
    env_file:
      - .env
    depends_on:
      - postgres  # depends on postgres so postgres will spin up first in its own container
    command: python ingestion/scheduler.py

  # spins up docker container for postgres. users wont need to download postgres manually now.
  postgres:
    image: postgres:15 # docker hub has application images that can be downloaded and used in a container
    env_file:
      - .env  
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    ports:
      - "5432:5432" # maps container port 5432 to host port 5432
    volumes:
      - postgres_data:/var/lib/postgresql/data # persist data between container restarts

volumes:
  postgres_data: # named volume to store postgres data permanently

# docker compose is the instructions on how to run the container. docker compose build will look here and build the container. this points to dockerfile.