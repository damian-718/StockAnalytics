version: '3.8'

services: # services refer to the scripts than run from docker-compose run. docker-compose run --rm compute-metrics. docker-compose run --rm process-candles
  compute-metrics:
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.spark # build the container using dockerfile.spark
    volumes:
      - ./spark:/app/spark # link code from local machine into docker container. edits locally edit the container.
      - ./.env:/app/.env
    env_file:
      - .env # load environment variables from .env
    environment:
      - SPARK_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569
      - PYTHONPATH=/app # environment variables for paths like python
    entrypoint: []
    command: # when the container starts, run the following command. this is how compute_metrics gets run.
      - /bin/bash
      - -c
      - |
        spark-submit \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569 \
          spark/jobs/compute_metrics.py

  process-candles:
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.spark
    volumes:
      - ./spark:/app/spark
      - ./.env:/app/.env
    env_file:
      - .env
    environment:
      - SPARK_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569
      - PYTHONPATH=/app
    entrypoint: []
    command: 
      - /bin/bash
      - -c
      - |
        spark-submit \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569,org.postgresql:postgresql:42.7.1 \
          spark/jobs/process_candles.py
          
  generate-report:
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.spark
    volumes:
      - ./spark:/app/spark
      - ./.env:/app/.env
    env_file:
      - .env
    environment:
      - SPARK_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569
      - PYTHONPATH=/app
    entrypoint: []
    command:  # spark-submit --packages will download maven packages into the docker container
      - /bin/bash
      - -c
      - |
        spark-submit \
          --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.569 \
          spark/jobs/generate_report.py

  ingest-candles: 
    build:
      context: .  # Changed from ..
      dockerfile: docker/Dockerfile.flask
    volumes: # propagate changes from local machine to docker container
      - ./app:/app/app
      - ./ingestion:/app/ingestion
      - ./.env:/app/.env
    env_file:
      - .env
    depends_on:
      - postgres  # depends on postgres so postgres will spin up first in its own container
    command: python ingestion/scheduler.py


  init-db:  # for table initilization for postgres to populate into
    build:
      context: .
      dockerfile: docker/Dockerfile.flask
    volumes:
      - ./app:/app/app
      - ./ingestion:/app/ingestion
      - ./.env:/app/.env
    env_file:
      - .env
    depends_on:
      - postgres
    command: >
      sh -c "
        python create_tables.py &&
        python create_spark_tables.py
      "

  # spins up docker container for postgres. users wont need to download postgres manually now.
  postgres:
    image: postgres:15 # docker hub has application images that can be downloaded and used in a container
    env_file:
      - .env  
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    ports:
      - "5432:5432" # maps container port 5432 to host port 5432
    volumes:
      - postgres_data:/var/lib/postgresql/data # persist data between container restarts

 # Airflow Services
  airflow-init:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-generate-one
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    depends_on:
      - postgres
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock  # Access Docker from Airflow
    command: version

  airflow-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    depends_on:
      - postgres
      - airflow-init
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-generate-one
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    depends_on:
      - postgres
      - airflow-init
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler

volumes:
  postgres_data: # named volume to store postgres data permanently. similar to a kubernetes pod volume. if container goes down, data is safely stored/persisted.


  # docker compose is the instructions on how to run the container. docker compose build will look here and build the container. this points to dockerfile.
  # docker compose can manage many container builds. tells it which dockerfile to use, what volume to link, and how to start it.
  # dockerfile builds container, docker compose tells it how to run, along with volumes etc..