import os
from datetime import datetime
from spark.spark_session import get_spark
from dotenv import load_dotenv
from pyspark.sql import functions as F
from pyspark.sql.window import Window
import anthropic

load_dotenv()

JOB_NAME = "generate_report"
S3_METRICS_PATH = os.getenv("S3_METRICS_PATH")
S3_REPORTS_PATH = os.getenv("S3_REPORTS_PATH")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

def analyze_with_claude(summary_stats): # summary stats is generated by run function and then passed in here for claude to analyze
    """Call Claude API to generate narrative from stats"""
    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    
    prompt = f"""You're a financial analyst. Analyze this stock market data and write a concise daily summary report.

    Data summary:
    {summary_stats}

    Write a professional 3-4 paragraph report covering:
    1. Overall market activity
    2. Notable movers and what might explain them
    3. Volatility patterns
    4. Any concerning trends

    Keep it factual and analytical."""
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return message.content[0].text

def run():
    spark = get_spark("GenerateReport")
    
    print(f"Reading metrics from: {S3_METRICS_PATH}")
    
    # read latest metrics file
    metrics_df = spark.read.parquet(S3_METRICS_PATH)
    
    # get most recent timestamp for each symbol
    window = Window.partitionBy("symbol").orderBy(F.desc("timestamp")) # a window is made for each ticker in the metrics file
    latest_df = ( # creates a dataframe of last seen price per ticker
        metrics_df
        .withColumn("rank", F.row_number().over(window))  # add a rank column for each row and assign a rownumber for each partition (ticker window)
        .filter(F.col("rank") == 1) # find only the latest row per parition (ordered by DESC, so 1 is newest)
        .drop("rank") # remove the rank column once we have the needed row per partition
    )
    
    # calculate summary statistics based on the dataframe created above
    summary = latest_df.agg( # latest_df has 1 row per ticker, but the latest 5min averages. Report is telling us details about the stock right now. current conditions.
        F.count("symbol").alias("total_stocks"),
        F.avg("vol_5").alias("avg_volatility"),
        F.sum(F.when(F.col("trend_signal") == 1, 1).otherwise(0)).alias("bullish_count"), # sum how many stocks have a trend signal of 1, meaning bullish
        F.sum(F.when(F.col("trend_signal") == -1, 1).otherwise(0)).alias("bearish_count")
    ).collect()[0] # collect is spark action, to do all computations here. will return the first row in the ew dataframe (which is just one row anyway)
    
    # get top movers by volatility
    top_volatile = ( # new dayaframe ordering from highest volatility as of now. vol_5 is a 5min window.
        latest_df
        .orderBy(F.desc("vol_5"))
        .limit(5)
        .select("symbol", "vol_5", "trend_signal", "ma_5", "ma_15")
        .collect()
    )
    
    # build summary text. summary is a dataframe which is now being formatted as text to be sent to claude as a string. summary['total_stocks'] returns the value at said column
    summary_stats = f"""
    Total Stocks Analyzed: {summary['total_stocks']} 
    Average Volatility: {summary['avg_volatility']:.4f}
    Bullish Stocks: {summary['bullish_count']}
    Bearish Stocks: {summary['bearish_count']}

    Top 5 Most Volatile Stocks:
    """
    # end of string ^

    # each row in the volatile dataframe is added as a new line to the summary stats string. It will have the totalstocks,avg volatility, bullish count, bearish count, and then each row from topvolatile along with it. 
    for row in top_volatile:
        summary_stats += f"\n- {row['symbol']}: vol={row['vol_5']:.4f}, trend={'bullish' if row['trend_signal']==1 else 'bearish' if row['trend_signal']==-1 else 'neutral'}, MA5={row['ma_5']:.2f}, MA15={row['ma_15']:.2f}"
    
    print("Generating AI analysis...")
    report_text = analyze_with_claude(summary_stats) # runs claude analysis which returns claudes message from prompt+summary_stats
    
    # create markdown report
    report_date = datetime.now().strftime("%Y-%m-%d")
    markdown_report = f"""# Daily Stock Market Summary
    **Date:** {report_date}

    ## AI Analysis
    {report_text}

    ## Raw Statistics
    {summary_stats}
    """
    # end of report ^

    # save to local file first
    report_filename = f"report_{report_date}.md" # markdown file saved to tmp folder
    with open(f"/tmp/{report_filename}", "w") as f:
        f.write(markdown_report)
    
    print(f"Report generated: {report_filename}")
    print(f"Uploading to: {S3_REPORTS_PATH}") 
    
    # upload to S3 using spark (convert to DataFrame for easy S3 write)
    report_df = spark.createDataFrame([{
        "date": report_date,
        "report": markdown_report,
        "total_stocks": summary['total_stocks'],
        "avg_volatility": float(summary['avg_volatility'])
    }])
    
    report_df.write.mode("append").parquet(S3_REPORTS_PATH)
    
    # also save the markdown file
    print(f"\nReport preview:\n{markdown_report}")
    
    spark.stop()
    print("Report generation complete.")

if __name__ == "__main__":
    run()